# -*- coding: utf-8 -*-
"""rembrandt_all.ipynb


Automatically generated by Colaboratory.


Original file is located at
   https://colab.research.google.com/drive/18ovn2QJa_DG6GiFDrT5jm9tEW6N0rw8Y
"""
#Importing libraries


import pandas as pd
import numpy as np
# Scikit-learn library: For SVM
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn import svm
# Matplotlib library to plot the charts
import matplotlib.mlab as mlab


metadata = pd.read_csv('metadata_for_ALL.csv')
metadata = metadata[["Unnamed: 0", "disease:ch1"]]
metadata.rename(columns={'Unnamed: 0':'sample_name'}, inplace=True)
metadata = metadata.set_index('sample_name', drop=False).rename_axis(None)
metadata = metadata.iloc[: , 1:]
metadata["disease:ch1"].value_counts()


df = pd.read_csv('expressions_for_ALL.csv')
df.rename(columns={'Unnamed: 0':'probe_id'}, inplace=True)
df_transposed = df.T
df_transposed = df_transposed.rename(columns=df_transposed.iloc[0])
df_transposed.drop(index=df_transposed.index[0], axis=0, inplace=True)


frames = [metadata, df_transposed]
df = pd.concat(frames, axis=1)


filtered = df.loc[df['disease:ch1'].isin(["glioblastoma multiforme", "astrocytoma", "oligodendroglioma", "normal"])]


df = filtered


import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
from sklearn.preprocessing import OrdinalEncoder


ord_enc = OrdinalEncoder()
df["disease:ch1"] = ord_enc.fit_transform(df[["disease:ch1"]])


X=df.drop(columns='disease:ch1')
y=df['disease:ch1']


X=X.astype()


from sklearn.preprocessing import StandardScaler
# get the features and label from the original dataframe
# performing standardization
sc = StandardScaler()
X_scaled = sc.fit_transform(X)


from sklearn.decomposition import PCA
pca = PCA(n_components = 0.85)
pca.fit(X_scaled)
print("Cumulative Variances (Percentage):")
print(np.cumsum(pca.explained_variance_ratio_ * 100))
components = len(pca.explained_variance_ratio_)
print(f'Number of components: {components}')
# Make the scree plot
plt.plot(range(1, components + 1), np.cumsum(pca.explained_variance_ratio_ * 100))
plt.xlabel("Number of components")
plt.ylabel("Explained variance (%)")


pca_components = abs(pca.components_)
print(pca_components)


# original_num_df the original numeric dataframe
# pca is the model
def create_importance_dataframe(pca, original_num_df):
   # Change pcs components ndarray to a dataframe
   importance_df  = pd.DataFrame(pca.components_)
   # Assign columns
   importance_df.columns  = original_num_df.columns
   # Change to absolute values
   importance_df =importance_df.apply(np.abs)
   # Transpose
   importance_df=importance_df.transpose()
   ## First get number of pcs
   num_pcs = importance_df.shape[1]
   ## Generate the new column names
   new_columns = [f'PC{i}' for i in range(1, num_pcs + 1)]
   ## Now rename
   importance_df.columns  =new_columns
   # Return importance df
   return importance_df


# Call function to create importance df
importance_df  =create_importance_dataframe(pca, X)


# Show first few rows
display(importance_df.head())


## PC1 top 20 important features
pc1_top_20_features = importance_df['PC1'].sort_values(ascending = False)[:20]
print(), print(f'PC1 top 20 features are \n')
display(pc1_top_20_features )


## PC2 top 10 important features
pc2_top_20_features = importance_df['PC2'].sort_values(ascending = False)[:20]
print(), print(f'PC2 top 20 features are \n')
display(pc2_top_20_features )


type(pc1_top_20_features)


pc1_top = pd.DataFrame({'probes':pc1_top_20_features.index, 'importance':pc1_top_20_features.values})
pc1_top


print('Top 20 most important features in each component')
print('===============================================')
for row in range(pca_components.shape[0]):
   # get the indices of the top 20 values in each row
   temp = np.argpartition(-(pca_components[row]), 20)
  
   # sort the indices in descending order
   indices = temp[np.argsort((-pca_components[row])[temp])][:20]
  
   # print the top 20 feature names
   print(f'Component {row}: {df.columns[indices].to_list()}')


print('Top 20 most important features in each component')
print('===============================================')
for row in range(pca_components.shape[0]):
   # get the indices of the top 20 values in each row
   temp = np.argpartition(-(pca_components[row]), 20)
  
   # sort the indices in descending order
   indices = temp[np.argsort((-pca_components[row])[temp])][:20]
  
   # print the top 20 feature names
   print(f'Component {row}: {df.columns[indices].to_list()}')


X_pca = pca.transform(X_scaled)


col=["disease:ch1"]
from sklearn.preprocessing import OneHotEncoder
df_new=pd.get_dummies(df,columns=col)
df_new['disease:ch1_0.0']


y_new= df_new[['disease:ch1_0.0','disease:ch1_1.0','disease:ch1_2.0','disease:ch1_3.0']]


from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.datasets import make_multilabel_classification
from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.multioutput import MultiOutputClassifier
from sklearn.pipeline import Pipeline


xtrain, xtest, ytrain, ytest=train_test_split(X_pca, y_new, train_size=0.6, random_state=88)
print(len(xtest))


classifier = MultiOutputClassifier(XGBClassifier())
clf = Pipeline([('classify', classifier)])
print (clf)


clf.fit(xtrain, ytrain)
print(clf.score(xtrain, ytrain))


yhat = clf.predict(xtest)
clf.score(xtrain, ytrain)


auc_y1 = roc_auc_score(ytest,yhat)


cr_y1 = classification_report(ytest,yhat,)


print (cr_y1)


# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D


df['pca-one'] = X_pca[:,0]
df['pca-two'] = X_pca[:,1]
df['pca-three'] = X_pca[:,2]


df['pca-one'].plot()


plt.figure(figsize=(16,10))
sns.scatterplot(
   x=df['pca-three'], y=df['pca-two'],
   hue=df['disease:ch1'],
   data=X_pca,
   legend="full",
)


ax = plt.figure(figsize=(16,10)).gca(projection='3d')
ax.scatter(
   zs=df['pca-three'] ,
   xs=df['pca-one'],
   ys=df['pca-two'] ,
   c=df['disease:ch1'],
   cmap='tab10'
)
ax.set_xlabel('pca-one')
ax.set_ylabel('pca-two')
ax.set_zlabel('pca-three')
plt.show()


time_start = time.time()
tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)
tsne_pca_results = tsne.fit_transform(X_pca)
print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))


tsne_pca_results.shape


df['tse-one'] = tsne_pca_results[:,0]
df['tse-two'] = tsne_pca_results[:,1]


sns.scatterplot(
   x=df['tse-one'], y=df['tse-two'],
   hue=df['disease:ch1'],
   data=df,
   legend="full",
   alpha=0.9,
)


ax = plt.figure(figsize=(16,10)).gca(projection='3d')
ax.scatter(
   xs=df['tse-one'] ,
   ys=df['tse-two'],
   c=df['disease:ch1'],
   cmap='tab10'
)
ax.set_xlabel('tse-one')
ax.set_ylabel('tse-two')
plt.show()


time_start = time.time()
umap = umap.UMAP(n_components=2, verbose=0)
umap_pca_results = umap.fit_transform(X_pca)
print('uMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))


umap_pca_results.shape


df['umap-one'] = umap_pca_results[:,0]
df['umap-two'] = umap_pca_results[:,1]


sns.scatterplot(
   x=df['umap-one'], y=df['umap-two'],
   hue=df['disease:ch1'],
   data=df,
   legend="full",
   alpha=0.9,
)


ax = plt.figure(figsize=(16,10)).gca(projection='3d')
ax.scatter(
   xs=df['umap-one'] ,
   ys=df['umap-two'],
   c=df['disease:ch1'],
   cmap='tab10'
)
ax.set_xlabel('umap-one')
ax.set_ylabel('umap-two')
plt.show()